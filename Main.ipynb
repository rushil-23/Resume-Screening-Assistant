{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95259606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONTACT INFO ===\n",
      "Email: nalekar.rushil23@gmail.com\n",
      "Phone: +31 639607724 \n",
      "Address: Marconistraat 5,Aalsmeer • +31 639607724 • nalekar.rushil23@gmail.com\n",
      "\n",
      "=== EDUCATION SECTION ===\n",
      "Bsc in Artificial Intelligence | Vrije Universitiet Amsterdam\n",
      "\n",
      "\n",
      "=== EXPERIENCE SECTION ===\n",
      "Teaching Assistant\n",
      "Vrije Universitiet Amsterdam\n",
      "\n",
      "Feb 2025 - Present\n",
      "\n",
      "Sep 2024 - Present\n",
      "\n",
      "Data Science Intern\n",
      "ReadLer\n",
      "\n",
      "Jul 2024- Sep 2024\n",
      "\n",
      "Marconistraat 5,Aalsmeer • +31 639607724 • nalekar.rushil23@gmail.com\n",
      "\n",
      "Software Enginerring Intern\n",
      "SHARP ENGINEERING\n",
      "\n",
      "\n",
      "=== SKILLS SECTION ===\n",
      "Specialization in Artificial Intelligence and Data Science.\n",
      "Projects on NLP, AI Agents, and Predictive Modeling.\n",
      "Strong foundation in Mathematics: Probability & Statistics, Calculus, and Linear Algebra.\n",
      "\n",
      "Assisted students with assignments, group sessions, and model-building projects. \n",
      "Clarified concepts in process modeling, causal graphs, and agent-based modeling\n",
      "Facilitating study groups to encourage peer learning and engagement.\n",
      "\n",
      "Technical Tools: Data Visualization, Data Analysis, SQL Databases, AI/ML Tools, Python,\n",
      "Java/JavaScript\n",
      "Languages: Dutch (A2), English (Fluent), Hindi (Native)\n",
      "Certifications: SQL Server for Data Analytics,Data Analytics with Python, Azure Cloud\n",
      "Computing & Data Management, Google Cloud Computing, Microsoft Power BI\n",
      "\n",
      "Customer Engagement & Product Expertise – Advised customers on product selection and\n",
      "styling by leveraging in-depth knowledge of Adidas products to enhance their shopping\n",
      "experience and drive sales.\n",
      "\n",
      "Built a JavaScript-based notification system to enhance communication efficiency. \n",
      "Collaborated on debugging and optimizing system performance for automation.\n",
      "\n",
      "Designed and implemented a Library Management System in C/C++ to manage catalogs and\n",
      "transactions, improving efficiency and usability.\n",
      "Streamlined operations, enhanced user experience, and supported digital transformation;\n",
      "received positive feedback from staff and students.\n",
      "\n",
      "\n",
      "=== PROJECTS SECTION ===\n",
      "Structured large phoneme datasets in JSON, refining AI speech evaluation and improving\n",
      "model interpretability. \n",
      " Developed a translation layer to convert AI-generated scores into real-world metrics,\n",
      "visualized using gradient-based bars. \n",
      " Analyzed and compared multiple AI speech models, optimizing accuracy and refining data\n",
      "processing workflows\n",
      "\n",
      "\n",
      "=== AWARDS SECTION ===\n",
      "Adidas Rookie of the Year – Recognized for exceptional performance, consistently exceeding\n",
      "targets and delivering impact in a fast-paced environment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "\n",
    "SECTION_LABELS = {\n",
    "    \"education\":  \"Academic history, degrees, universities, schools, coursework, graduation years.\",\n",
    "    \"experience\": \"Work history, job titles, internships, responsibilities, achievements at companies.\",\n",
    "    \"skills\":     \"Technical skills, programming languages, tools, frameworks, platforms, soft skills.\",\n",
    "    \"projects\":   \"Descriptions of projects, systems built, responsibilities within projects, outcomes.\",\n",
    "    \"awards\":     \"Honors, certifications, distinctions, recognitions, prizes.\",\n",
    "    \"hobbies\":    \"Personal interests, extracurricular activities, volunteering, non-work hobbies.\"\n",
    "}\n",
    "\n",
    "# Load models once\n",
    "SEM_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "semantic_model = SentenceTransformer(SEM_MODEL_NAME)\n",
    "zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "section_keys = list(SECTION_LABELS.keys())\n",
    "section_embeddings = semantic_model.encode(list(SECTION_LABELS.values()))\n",
    "\n",
    "# -------------------- UTILITIES --------------------\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Lowercase & collapse non-alphanumerics for stable hashing/compare.\"\"\"\n",
    "    return re.sub(r\"\\W+\", \" \", s.lower()).strip()\n",
    "\n",
    "def jaccard(a: str, b: str) -> float:\n",
    "    \"\"\"Simple Jaccard similarity on word sets.\"\"\"\n",
    "    sa = set(normalize_text(a).split())\n",
    "    sb = set(normalize_text(b).split())\n",
    "    if not sa or not sb:\n",
    "        return 0.0\n",
    "    return len(sa & sb) / len(sa | sb)\n",
    "\n",
    "def is_near_duplicate(existing_texts, candidate, jaccard_threshold=0.85):\n",
    "    \"\"\"Check if candidate is near-duplicate of any already kept text.\"\"\"\n",
    "    n = normalize_text(candidate)\n",
    "    h = hashlib.md5(n.encode(\"utf-8\")).hexdigest()\n",
    "    if h in existing_texts[\"hashes\"]:\n",
    "        return True\n",
    "    # quick jaccard pass vs up to N prior samples (avoid O(n^2) blowup)\n",
    "    for prev in existing_texts[\"samples\"][-50:]:\n",
    "        if jaccard(prev, candidate) >= jaccard_threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def mark_kept(existing_texts, text):\n",
    "    n = normalize_text(text)\n",
    "    h = hashlib.md5(n.encode(\"utf-8\")).hexdigest()\n",
    "    existing_texts[\"hashes\"].add(h)\n",
    "    existing_texts[\"samples\"].append(text)\n",
    "\n",
    "# -------------------- PDF → BLOCKS --------------------\n",
    "\n",
    "def extract_text_blocks(pdf_path):\n",
    "    \"\"\"\n",
    "    Use PyMuPDF 'blocks' to get reasonably distinct chunks.\n",
    "    Filters tiny/noisy blocks. Merges lines in the same block.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    blocks = []\n",
    "    for page in doc:\n",
    "        for x0, y0, x1, y1, text, _, btype in page.get_text(\"blocks\"):\n",
    "            t = (text or \"\").strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            # Filter tiny/noisy blocks (e.g., single token footers)\n",
    "            if len(t.split()) < 4:\n",
    "                continue\n",
    "            # Compact internal whitespace\n",
    "            t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "            t = re.sub(r\"\\n{2,}\", \"\\n\", t)\n",
    "            blocks.append(t)\n",
    "    return blocks\n",
    "\n",
    "# -------------------- CONTACT INFO --------------------\n",
    "\n",
    "def extract_contact(full_text):\n",
    "    email = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", full_text)\n",
    "    phone = re.search(r\"\\+?\\d[\\d\\s\\-]{8,}\", full_text)\n",
    "    linkedin = re.search(r\"https?://(www\\.)?linkedin\\.com/[^\\s]+\", full_text)\n",
    "\n",
    "    # Heuristic address: look for a line with a number + a street-like token / city token\n",
    "    address = None\n",
    "    for line in full_text.splitlines():\n",
    "        l = line.strip()\n",
    "        if re.search(r\"\\d{1,5}\\s+\\S+\", l) and re.search(r\"(street|straat|road|lane|avenue|weg|plein|laan|city|amsterdam|nl|ind|uk|usa|us)\\b\", l, re.IGNORECASE):\n",
    "            address = l\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"email\": email.group() if email else None,\n",
    "        \"phone\": phone.group() if phone else None,\n",
    "        \"linkedin\": linkedin.group() if linkedin else None,\n",
    "        \"address\": address,\n",
    "    }\n",
    "\n",
    "# -------------------- CLASSIFICATION --------------------\n",
    "\n",
    "def classify_block(block: str) -> str:\n",
    "    \"\"\"Ensemble: zero-shot vs semantic similarity; pick higher confidence.\"\"\"\n",
    "    # Zero-shot\n",
    "    zs = zero_shot(block, section_keys)\n",
    "    zs_label = zs[\"labels\"][0]\n",
    "    zs_score = zs[\"scores\"][0]\n",
    "\n",
    "    # Semantic similarity\n",
    "    emb = semantic_model.encode([block])\n",
    "    sims = cosine_similarity(emb, section_embeddings)[0]\n",
    "    sem_idx = int(np.argmax(sims))\n",
    "    sem_label = section_keys[sem_idx]\n",
    "    sem_score = sims[sem_idx]\n",
    "\n",
    "    # Winner (with small tie-break towards zero-shot)\n",
    "    if abs(zs_score - sem_score) < 0.05:\n",
    "        return zs_label\n",
    "    return zs_label if zs_score > sem_score else sem_label\n",
    "\n",
    "def post_rule(block: str, label: str) -> str:\n",
    "    \"\"\"Hard rules to fix common misclassifications.\"\"\"\n",
    "    low = block.lower()\n",
    "\n",
    "    # Strong education cues\n",
    "    if re.search(r\"\\b(bsc|msc|phd|b\\.tech|m\\.tech|bachelor|master|university|universiteit|degree|diploma|coursework)\\b\", low):\n",
    "        return \"education\"\n",
    "\n",
    "    # Strong experience cues (titles, companies, date ranges)\n",
    "    if re.search(r\"\\b(intern|assistant|engineer|developer|analyst|associate|consultant|manager|present|worked at|company)\\b\", low):\n",
    "        return \"experience\"\n",
    "    if re.search(r\"\\b(20\\d{2}\\s*[-–]\\s*(present|20\\d{2}))\\b\", low):\n",
    "        return \"experience\"\n",
    "\n",
    "    # Skills: long comma/semicolon lists, tool stacks\n",
    "    if (low.count(\",\") + low.count(\";\")) >= 4 or re.search(r\"\\bpython|java|c\\+\\+|sql|power bi|azure|gcp|aws|pandas|numpy|sklearn|tensorflow|pytorch|tableau\\b\", low):\n",
    "        return \"skills\"\n",
    "\n",
    "    # Projects\n",
    "    if re.search(r\"\\bproject(s)?\\b\", low) and len(low) > 60:\n",
    "        return \"projects\"\n",
    "\n",
    "    # Awards/certs\n",
    "    if re.search(r\"\\b(award|honor|prize|certification|certified|recognition)\\b\", low):\n",
    "        return \"awards\"\n",
    "\n",
    "    # Hobbies\n",
    "    if re.search(r\"\\b(hobbies|interests|extracurricular|volunteer)\\b\", low):\n",
    "        return \"hobbies\"\n",
    "\n",
    "    return label\n",
    "\n",
    "# -------------------- DEDUP / OVERLAP PREVENTION --------------------\n",
    "\n",
    "def deduplicate_sections(sections: dict, jaccard_threshold: float = 0.85) -> dict:\n",
    "    \"\"\"\n",
    "    Remove duplicates/near-duplicates across ALL sections.\n",
    "    - Uses hash of normalized text for exact dupes.\n",
    "    - Uses Jaccard similarity for near dupes.\n",
    "    - Keeps the first occurrence encountered (by section order below).\n",
    "    \"\"\"\n",
    "    ordered_sections = [\"education\", \"experience\", \"skills\", \"projects\", \"awards\", \"hobbies\"]\n",
    "    existing = {\"hashes\": set(), \"samples\": []}\n",
    "    cleaned = {k: [] for k in ordered_sections}\n",
    "\n",
    "    # enforce a stable section iteration; include any unexpected keys too\n",
    "    all_keys = [s for s in ordered_sections if s in sections] + [k for k in sections.keys() if k not in ordered_sections]\n",
    "\n",
    "    for sec in all_keys:\n",
    "        for block in sections[sec]:\n",
    "            if not block or len(block.strip()) < 5:\n",
    "                continue\n",
    "            if is_near_duplicate(existing, block, jaccard_threshold=jaccard_threshold):\n",
    "                continue\n",
    "            cleaned.setdefault(sec, []).append(block)\n",
    "            mark_kept(existing, block)\n",
    "\n",
    "    # Drop empty sections for neatness\n",
    "    return {k: v for k, v in cleaned.items() if v}\n",
    "\n",
    "# -------------------- MAIN PIPELINE --------------------\n",
    "\n",
    "def parse_resume(pdf_path: str, dedupe_jaccard: float = 0.85):\n",
    "    # 1) Blocks from PDF\n",
    "    blocks = extract_text_blocks(pdf_path)\n",
    "    full_text = \"\\n\".join(blocks)\n",
    "\n",
    "    # 2) Contact info (before anything else)\n",
    "    contact = extract_contact(full_text)\n",
    "\n",
    "    # 3) Classify each block with ensemble + post-rules\n",
    "    raw_sections = defaultdict(list)\n",
    "    for block in blocks:\n",
    "        label = classify_block(block)\n",
    "        fixed = post_rule(block, label)\n",
    "        raw_sections[fixed].append(block)\n",
    "\n",
    "    # 4) De-duplicate across sections\n",
    "    sections = deduplicate_sections(raw_sections, jaccard_threshold=dedupe_jaccard)\n",
    "\n",
    "    # 5) Print nicely\n",
    "    print(\"\\n=== CONTACT INFO ===\")\n",
    "    for k, v in contact.items():\n",
    "        if v:\n",
    "            print(f\"{k.capitalize()}: {v}\")\n",
    "\n",
    "    for sec, items in sections.items():\n",
    "        print(f\"\\n=== {sec.upper()} SECTION ===\")\n",
    "        for it in items:\n",
    "            print(it)\n",
    "            print()\n",
    "\n",
    "# -------------------- RUN --------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse_resume(\"Rushil-CV.pdf\", dedupe_jaccard=0.86)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
